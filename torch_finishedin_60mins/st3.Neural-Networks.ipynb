{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d49a477",
   "metadata": {},
   "source": [
    "# 신경망\n",
    "* torch.nn을 이용하여 신경망 생성해본다.\n",
    "* 숫자 이미지를 분류하는 신경망을 예제로 확인\n",
    "\n",
    "\n",
    "## 1. 신경망을 정의\n",
    "* nn은 모델을 정의하고 미분하는데 autograd를 내부적으로 사용한다.\n",
    "* nn.module은 layer와 output 반환하는 forward(input) 메서드를 포함한다.\n",
    "\n",
    "일반적인 학습과정의 순서:\n",
    "* 매개변수(weight, bias)를 갖는 신경망 정의\n",
    "* dataset 입력의 반복\n",
    "* forward process\n",
    "* loss 계산\n",
    "* graident의 back probagation\n",
    "* weight 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6d276a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "995aaefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "#         super(Net, self).__init__()  # 이게 정석\n",
    "        super().__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # 위와 같음\n",
    "        x = torch.flatten(x, 1) # 1차원 기준 벡터로 flatten !!! # (1, 400)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58958184",
   "metadata": {},
   "source": [
    "## 2 매개변수(weight, bias)의 반환이 가능하다.\n",
    "매개 변수를 출력해보자.\n",
    "* `list(model.parameters())`로 리스트로 받거나\n",
    "* `model.named_parameters()`로 딕셔너리로 받을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1aaadd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters()) # generator기 때문에 list화로 리스트로 반환 가능하다.\n",
    "print(len(params))\n",
    "print(params[0].shape) # conv1의 .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e10dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.float32 torch.Size([6, 1, 5, 5])\n",
      "conv1.bias torch.float32 torch.Size([6])\n",
      "conv2.weight torch.float32 torch.Size([16, 6, 5, 5])\n",
      "conv2.bias torch.float32 torch.Size([16])\n",
      "fc1.weight torch.float32 torch.Size([120, 400])\n",
      "fc1.bias torch.float32 torch.Size([120])\n",
      "fc2.weight torch.float32 torch.Size([84, 120])\n",
      "fc2.bias torch.float32 torch.Size([84])\n",
      "fc3.weight torch.float32 torch.Size([10, 84])\n",
      "fc3.bias torch.float32 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in net.named_parameters():\n",
    "    print(name, parameter.dtype, parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af0931",
   "metadata": {},
   "source": [
    "## 3. 입력값을 넣어 forward, backward를 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "349fe4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 배치를 고려해야 한다.\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec974f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward\n",
    "net.zero_grad()\n",
    "# output이 scalar가 아니므로 값의 shape의 벡터를 전달\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a87ff",
   "metadata": {},
   "source": [
    "!주의:\n",
    "* `torch.nn`은 미니배치만 지원! 즉, 하나의 샘플 아닌 미니 배치를 입력으로 받는다. 따라서 `conv2d`는 (B x C x H x W)를 입력받는다.\n",
    "* 만약 하나의 샘플만 있다면 `tensor.unsqueeze(0`로 한 차원 추가 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b22d8c",
   "metadata": {},
   "source": [
    "## 4. loss function\n",
    "* `nn.MSELoss`로 계산해보자.\n",
    "* torch.view(shape) : np의 reshape과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa3425d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7115, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10) # 임의의 정답값이라 가정\n",
    "target = target.view(1, -1)  # 한차원 늘림. (unsquieeze)와도 동일 하다.\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2a5b7",
   "metadata": {},
   "source": [
    "* `.grad_fn` 속성으로 loss를 역방향 추적하면 이러한 모습의 연산그래프\n",
    "```\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> flatten -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efbfca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MseLossBackward0 at 0x7f619b12e4c0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn  # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7aefdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddmmBackward0 at 0x7f619b12dc10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions[0][0] # Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1678a527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AccumulateGrad at 0x7f619b143760>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions[0][0].next_functions[0][0]  # relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9fb598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bf349c0",
   "metadata": {},
   "source": [
    "## 5. backprob, 역전파\n",
    "* loss.backward()시, 전체 DAG가 미분되며, 그래프내의 requires_grad=True 인모든 텐서들은 gradient가 누적된 `.grad`를 갖게된다.\n",
    "* `loss.backward()로 역전파 전과 후의 conv1의 biase 변수의 변화도 확인`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f9df19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "역전파 전의 conv1 grad\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "역전파 후의 conv1 grad\n",
      "tensor([-0.0403,  0.0148,  0.0027, -0.0133,  0.0080, -0.0215])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad() # 모든 매개변수 gradient를 0으로 초기화\n",
    "print('역전파 전의 conv1 grad')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "print('역전파 후의 conv1 grad')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7f753",
   "metadata": {},
   "source": [
    "## 6. 가중치 갱신\n",
    "optimizer로 진행. SGD, Nesterov-SGD, Adam, RMSProp 등이 `torch.optim`에 정의 되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e8b562b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 아래는  traning loop에 구현해야 함.\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214daba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99686c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeb0a02d",
   "metadata": {},
   "source": [
    "--------------\n",
    "### 실험.\n",
    "grad는 덮어쓰기가 아니라 축적 연산이다?!\n",
    "\n",
    "```python\n",
    "grad = grad # (X)\n",
    "grad += grad # (O)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7124b1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.1727, -0.3400, -1.0943,  ...,  1.4351, -2.2209, -1.9274],\n",
       "           [ 0.5412, -0.9375, -0.6681,  ...,  0.1110,  0.2013, -0.2840],\n",
       "           [-0.1223, -1.8497, -1.0430,  ...,  0.4606,  1.6043, -1.6630],\n",
       "           ...,\n",
       "           [ 0.0416, -0.9585, -0.1484,  ...,  0.4781,  1.9137, -1.1823],\n",
       "           [-0.5759,  0.0422, -0.0964,  ...,  0.1008, -0.0169,  0.4571],\n",
       "           [ 0.5812,  0.1864,  1.2260,  ...,  0.1657, -1.1614,  0.6674]]]]),\n",
       " tensor([[-0.5966,  0.1820, -0.8567,  1.1006, -1.0712,  0.1227, -0.5663,  0.3731,\n",
       "          -0.8920, -1.5091]]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch copy 방법\n",
    "input2 = torch.ones_like(input).copy_(input)\n",
    "target2 = torch.ones_like(target).copy_(target)\n",
    "\n",
    "input2, target2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88cb42d",
   "metadata": {},
   "source": [
    "실험:\n",
    "* net은 zero_grad 없이 두번연속 backwward.\n",
    "* net_copy는 zero_grad를 넣어 초기화함.\n",
    "\n",
    "결과:\n",
    "* net의 최종 grad는 net_copy grad의 두배. 즉, 누적됨을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e6537e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 역전파 후의 conv1 grad\n",
      "tensor([-0.0017, -0.0135,  0.0020, -0.0056, -0.0104,  0.0008])\n",
      "tensor([-0.0017, -0.0135,  0.0020, -0.0056, -0.0104,  0.0008])\n",
      "두번째 역전파 후의 conv1 grad\n",
      "tensor([-0.0035, -0.0269,  0.0041, -0.0112, -0.0208,  0.0016])\n",
      "tensor([-0.0017, -0.0135,  0.0020, -0.0056, -0.0104,  0.0008])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "net = Net()\n",
    "net_copy = copy.deepcopy(net)\n",
    "output = net(input2)\n",
    "loss = criterion(output, target2)\n",
    "loss.backward()\n",
    "\n",
    "output = net_copy(input2)\n",
    "loss = criterion(output, target2)\n",
    "loss.backward()\n",
    "\n",
    "print('첫번째 역전파 후의 conv1 grad')\n",
    "print(net.conv1.bias.grad)\n",
    "print(net_copy.conv1.bias.grad)\n",
    "\n",
    "output = net(input2)\n",
    "loss = criterion(output, target2)\n",
    "loss.backward()\n",
    "\n",
    "net_copy.zero_grad()\n",
    "output = net_copy(input2)\n",
    "loss = criterion(output, target2)\n",
    "loss.backward()\n",
    "print('두번째 역전파 후의 conv1 grad')\n",
    "print(net.conv1.bias.grad)\n",
    "print(net_copy.conv1.bias.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0b91b690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0182, -0.0053, -0.0224, -0.0109, -0.0335,  0.0049])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_copy.conv1.bias.grad * 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7f2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91733f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57bab145",
   "metadata": {},
   "source": [
    "# 참고 :\n",
    "* NN구성하는 layer, loss 함수들 doc : https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8009c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
